# ğŸ­ Multi-modal Emotion Recognition System

## ğŸš€ Project Overview

This project is an advanced **Multi-modal Emotion Recognition System** that intelligently fuses video, audio, and text inputs to predict human emotions, including **Happy**, **Sad**, **Angry**, and **Neutral**. Itâ€™s built with state-of-the-art technologies like **PyTorch**, **Transformers**, **OpenCV**, and **Librosa**, wrapped in a simple **Streamlit** interface for easy interaction.

---

## âœ… Features

- âœ… Video emotion processing using facial keypoints
- âœ… Audio emotion extraction with MFCC features
- âœ… Text emotion analysis using BERT embeddings
- âœ… Multi-modal fusion model combining all three inputs
- âœ… Real-time interactive prediction web app
- âœ… Configurable via a clean `config.yaml` file
- âœ… Easily extendable for custom datasets

---

## ğŸ“¸ Screenshot

Hereâ€™s what the app looks like in action:

!(https://res.cloudinary.com/de1tywvqm/image/upload/v1757834533/Screenshot_2025-09-14_123705_vcvnhm.png)

---

## âš™ï¸ Technologies Used

| Feature             | Technology / Library           |
|---------------------|--------------------------------|
| Video Processing    | OpenCV                         |
| Audio Processing    | Librosa                        |
| Text Processing     | HuggingFace Transformers (BERT) |
| Deep Learning Model | PyTorch                        |
| UI / Deployment     | Streamlit                      |
| Config Management   | PyYAML                         |

---

## ğŸ—ï¸ Project Structure

```plaintext
multi_modal_emotion_recognition/
â”‚
â”œâ”€â”€ config.yaml
â”œâ”€â”€ data/
â”œâ”€â”€ models/
â”œâ”€â”€ utils/
â”œâ”€â”€ app/
â”œâ”€â”€ train.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
````

---

## âš¡ Setup Instructions

1ï¸âƒ£ Clone the project:

```bash
git clone <your-repo-url>
cd multi_modal_emotion_recognition
```

2ï¸âƒ£ Create and activate a virtual environment:

```bash
python -m venv .venv
.\.venv\Scripts\activate  # Windows
# OR
source .venv/bin/activate  # macOS/Linux
```

3ï¸âƒ£ Install dependencies:

```bash
pip install -r requirements.txt
```

4ï¸âƒ£ Run the Streamlit app:

```bash
streamlit run multi_modal_emotion_recognition/app/streamlit_app.py
```

---

## ğŸš€ How to Use

1. Open browser at `http://localhost:8501`.
2. Upload a sample video (.mp4), audio (.wav), and input some text.
3. Click **Predict Emotion** to see the predicted emotion and confidence.

---

## ğŸ“ Sample Config (`config.yaml`)

```yaml
paths:
  video_sample: "./data/video/sample_video.mp4"
  audio_sample: "./data/audio/sample_audio.wav"
  text_sample: "./data/text/sample_text.txt"

parameters:
  frame_rate: 1
  text_max_length: 128
  num_classes: 4
```

---

## âš ï¸ Notes

* Currently uses dummy random tensors in inference until real training and weight saving are implemented.
* Ensure proper file paths are configured in `config.yaml`.
* Trained model weight saving and loading will be added soon.

---

## ğŸ“š Future Improvements

* Full dataset preprocessing pipeline
* Real training loop for fusion model
* Model weight-saving/loading system
* Enhanced visualization of prediction probabilities
* Docker support for deployment

---

## ğŸ’¡ License

MIT License

---

Made with â¤ï¸ by Anantha Kumar G

```

```
